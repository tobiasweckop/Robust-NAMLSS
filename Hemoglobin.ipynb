{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "\n",
    "class NormalNAMLSS(nn.Module):\n",
    "    def __init__(self, n_covariates, hidden_size=8, intercept=False):\n",
    "        super(NormalNAMLSS, self).__init__()\n",
    "\n",
    "        self.submodules = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, hidden_size),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size, 2)\n",
    "            ) for _ in range(n_covariates)\n",
    "        ])\n",
    "\n",
    "        self.use_intercept = intercept\n",
    "        if self.use_intercept:\n",
    "            self.intercept = nn.Parameter(torch.zeros(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        param_mat_list = [self.submodules[i](x[:, i:i + 1]) for i in range(x.shape[1])]\n",
    "        x = torch.stack(param_mat_list, dim=1)\n",
    "        mu_components = x[:, :, 0]\n",
    "        sigma_components = F.softplus(x[:, :, 1])\n",
    "\n",
    "        mu = torch.sum(mu_components, dim=1).unsqueeze(dim=1)\n",
    "        sigma = torch.sum(sigma_components, dim=1).unsqueeze(dim=1)\n",
    "\n",
    "        if self.use_intercept:\n",
    "            mu = mu + self.intercept[0]\n",
    "            sigma = sigma + F.softplus(self.intercept[1])\n",
    "\n",
    "        return mu, sigma\n",
    "\n",
    "    def nll_loss(self, mu, sigma, y_true, robustness_factor=None):\n",
    "        normal_dist = dist.Normal(mu, sigma)\n",
    "        # log_likelihood = normal_dist.log_prob(y_true).sum()\n",
    "        log_likelihood = normal_dist.log_prob(y_true).mean()\n",
    "\n",
    "        if robustness_factor is not None:\n",
    "            # log_likelihood = torch.log((1 + torch.exp(normal_dist.log_prob(y_true) + robustness_factor)) / (1 + torch.exp(robustness_factor))).sum()\n",
    "            log_likelihood = torch.log((1 + torch.exp(normal_dist.log_prob(y_true) + robustness_factor)) / (1 + torch.exp(robustness_factor))).mean()\n",
    "\n",
    "        nll = -log_likelihood\n",
    "        return nll\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None, n_epochs=10000, lr=1e-3, weight_decay=0.0, \n",
    "            early_stopping_patience=10, robustness_factor=None):\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            self.train()\n",
    "\n",
    "            # Forward pass and loss computation\n",
    "            mu, sigma = self.forward(X_train)\n",
    "            train_loss = self.nll_loss(mu, sigma, y_train, robustness_factor)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            val_loss = None\n",
    "            if X_val is not None and y_val is not None:\n",
    "                self.eval()\n",
    "                with torch.no_grad():\n",
    "                    mu_val, sigma_val = self.forward(X_val)\n",
    "                    val_loss = self.nll_loss(mu_val, sigma_val, y_val, robustness_factor).item()\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_model_state = self.state_dict()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if (patience_counter >= early_stopping_patience) and (epoch >= 1000):\n",
    "                    print(f\"Early stopping at epoch {epoch}. Best validation loss: {best_val_loss:.4f}\")\n",
    "                    self.load_state_dict(best_model_state)\n",
    "                    break\n",
    "\n",
    "            if epoch % 100 == 0 or val_loss is not None:\n",
    "                # print(f\"Epoch {epoch} - Train Loss: {train_loss.item():.4f} - Val Loss: {val_loss:.4f}\" if val_loss else f\"Epoch {epoch} - Train Loss: {train_loss.item():.4f}\")\n",
    "                pass\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        mu, sigma = self.forward(x)\n",
    "\n",
    "        mu = mu.detach()\n",
    "        sigma = sigma.detach()\n",
    "\n",
    "        return mu, sigma\n",
    "        \n",
    "\n",
    "    def marginal_effects(self, x):\n",
    "        with torch.no_grad():\n",
    "            param_mat_list = [self.submodules[i](x[:, i:i + 1]) for i in range(x.shape[1])]\n",
    "            x = torch.stack(param_mat_list, dim=1)\n",
    "            mu_components = x[:, :, 0].detach().cpu().numpy()\n",
    "            sigma_components = F.softplus(x[:, :, 1]).detach().cpu().numpy()\n",
    "        return mu_components, sigma_components"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
