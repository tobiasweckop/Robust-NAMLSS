{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "class GammaNAMLSS(nn.Module):\n",
    "    def __init__(self, n_covariates, hidden_size=8, intercept=False):\n",
    "        super(GammaNAMLSS, self).__init__()\n",
    "\n",
    "        self.submodules = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, hidden_size),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size, 2)\n",
    "            ) for _ in range(n_covariates)\n",
    "        ])\n",
    "\n",
    "        self.use_intercept = intercept\n",
    "        if self.use_intercept:\n",
    "            self.intercept = nn.Parameter(torch.zeros(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        param_mat_list = [self.submodules[i](x[:, i:i + 1]) for i in range(x.shape[1])]\n",
    "        x = torch.stack(param_mat_list, dim=1)\n",
    "        alpha_components = F.softplus(x[:, :, 0])\n",
    "        beta_components = F.softplus(x[:, :, 1])\n",
    "\n",
    "        alpha = torch.sum(alpha_components, dim=1).unsqueeze(dim=1) + 1e-10\n",
    "        beta = torch.sum(beta_components, dim=1).unsqueeze(dim=1) + 1e-10\n",
    "\n",
    "        if self.use_intercept:\n",
    "            alpha = alpha + F.softplus(self.intercept[0])\n",
    "            beta = beta + F.softplus(self.intercept[1])\n",
    "\n",
    "        return alpha, beta\n",
    "\n",
    "\n",
    "    def nll_loss(self, alpha, beta, y_true, robustness_factor=None):\n",
    "        # gamma_dist = dist.Gamma(alpha, beta)\n",
    "        # log_likelihood = gamma_dist.log_prob(y_true)\n",
    "\n",
    "        # log_likelihood = (alpha - 1) * torch.log(y_true) - beta * y_true - torch.lgamma(alpha) + alpha * torch.log(beta)\n",
    "        log_likelihood = torch.xlogy(alpha - 1, y_true) - beta * y_true - torch.lgamma(alpha) + alpha * torch.log(beta)\n",
    "\n",
    "        if robustness_factor is not None:\n",
    "            log_likelihood = torch.log((1 + torch.exp(log_likelihood + robustness_factor)) / (1 + torch.exp(robustness_factor)))\n",
    "\n",
    "        nll = -log_likelihood.mean()\n",
    "        return nll\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val = None, y_val = None, n_epochs = 10000, lr = 1e-3, weight_decay = 0.0, \n",
    "            early_stopping_patience = 10, robustness_factor = None, gradient_clip_value = 1.0):\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            self.train()\n",
    "\n",
    "            # Forward pass and loss computation\n",
    "            alpha, beta = self.forward(X_train)\n",
    "            train_loss = self.nll_loss(alpha, beta, y_train, robustness_factor)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "\n",
    "            # utils.clip_grad_norm_(self.parameters(), gradient_clip_value)\n",
    "\n",
    "            max_grad = max(p.grad.abs().max().item() if p.grad is not None else 0 for p in self.parameters())\n",
    "            mean_grad = sum(p.grad.abs().mean().item() if p.grad is not None else 0 for p in self.parameters()) / len(list(self.parameters()))\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch} - Train Loss: {train_loss.item():.4f} - Max Grad: {max_grad:.4f} - Mean Grad: {mean_grad:.4f}\")\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            val_loss = None\n",
    "            if X_val is not None and y_val is not None:\n",
    "                self.eval()\n",
    "                with torch.no_grad():\n",
    "                    alpha_val, beta_val = self.forward(X_val)\n",
    "                    val_loss = self.nll_loss(alpha_val, beta_val, y_val, robustness_factor).item()\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_model_state = self.state_dict()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if (patience_counter >= early_stopping_patience) and (epoch >= 1000):\n",
    "                    print(f\"Early stopping at epoch {epoch}. Best validation loss: {best_val_loss:.4f}\")\n",
    "                    self.load_state_dict(best_model_state)\n",
    "                    break\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch} - Train Loss: {train_loss.item():.4f} - Val Loss: {val_loss:.4f}\" if val_loss else f\"Epoch {epoch} - Train Loss: {train_loss.item():.4f}\")\n",
    "                \n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        alpha, beta = self.forward(x)\n",
    "\n",
    "        alpha = alpha.detach()\n",
    "        beta = beta.detach()\n",
    "\n",
    "        return alpha, beta\n",
    "        \n",
    "\n",
    "    def marginal_effects(self, x):\n",
    "        with torch.no_grad():\n",
    "            param_mat_list = [self.submodules[i](x[:, i:i + 1]) for i in range(x.shape[1])]\n",
    "            x = torch.stack(param_mat_list, dim=1)\n",
    "            alpha_components = F.softplus(x[:, :, 0]).detach().cpu().numpy()\n",
    "            beta_components = F.softplus(x[:, :, 1]).detach().cpu().numpy()\n",
    "\n",
    "            mean_components = alpha_components * beta_components\n",
    "            variance_components = alpha_components * beta_components ** 2\n",
    "\n",
    "        return mean_components, variance_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# parking_df = pd.read_csv(\"C:/Users/Tobias/Desktop/Workspace/Robust Neural Networks/Pytorch-Workspace/parking_ohe.csv\", sep = \";\")\n",
    "parking_df = pd.read_csv(\"parking_sample.csv\", sep = \";\")\n",
    "parking_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "parking = parking_df.to_numpy(dtype=float)\n",
    "parking[:,0] = parking[:,0]/3600 # Konvertiert Sekunden in Stunden\n",
    "\n",
    "train_indices = random.sample(range(1, len(parking)), 10000)\n",
    "val_indices = random.sample(range(1, len(parking)), 10000)\n",
    "test_indices = random.sample(range(1, len(parking)), 10000)\n",
    "\n",
    "train_sample = parking[train_indices,:]\n",
    "val_sample = parking[val_indices,:]\n",
    "test_sample = parking[test_indices,:]\n",
    "\n",
    "plt.plot(train_sample[:,2], np.log(train_sample[:,0]), \"o\", markersize = 1)\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"log(Parking Duration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(train_sample[:,range(2,4)], dtype = torch.float32) # range interval is [a,b)\n",
    "y_train = torch.tensor(train_sample[:,0], dtype = torch.float32)\n",
    "\n",
    "X_val = torch.tensor(val_sample[:,range(2,4)], dtype = torch.float32)\n",
    "y_val = torch.tensor(val_sample[:,0], dtype = torch.float32)\n",
    "\n",
    "X_test = torch.tensor(test_sample[:,range(2,4)], dtype = torch.float32)\n",
    "y_test = torch.tensor(test_sample[:0], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = torch.tensor(scaler.fit_transform(X_train), dtype = torch.float32)\n",
    "X_val_scaled = torch.tensor(scaler.transform(X_val), dtype = torch.float32)\n",
    "X_test_scaled = torch.tensor(scaler.transform(X_test), dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "gamma_namlss = GammaNAMLSS(X_train.shape[1])\n",
    "\n",
    "with cProfile.Profile() as pr:\n",
    "    gamma_namlss.fit(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "\n",
    "stats = pstats.Stats(pr)\n",
    "stats.strip_dirs().sort_stats(\"cumulative\").print_stats(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma_namlss = GammaNAMLSS(X_train.shape[1])\n",
    "# gamma_namlss.fit(X_train_scaled, y_train, X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha, beta = gamma_namlss.predict(X_test_scaled)\n",
    "# gamma_dist = dist.Gamma(alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import gamma\n",
    "\n",
    "# quantiles = gamma.ppf([0.025, 0.975], alpha, scale = 1/beta)\n",
    "# quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower = quantiles[:,0]\n",
    "# upper = quantiles[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# sorted_indices = np.argsort(X_test[:,0])\n",
    "# X_test_sorted = X_test[sorted_indices, 0]\n",
    "# y_test_sorted = y_test[sorted_indices]\n",
    "\n",
    "# plt.plot(X_test_sorted, y_test_sorted, \"o\", markersize = 1)\n",
    "# plt.plot(X_test_sorted, lower[sorted_indices], color = \"red\")\n",
    "# plt.plot(X_test_sorted, upper[sorted_indices], color = \"red\")\n",
    "# plt.ylim((0, 50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robust_gamlss = GammaNAMLSS(X_train.shape[1])\n",
    "# robust_gamlss.fit(X_train_scaled, y_train, X_val_scaled, y_val, robustness_factor = torch.tensor(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robust_alpha, robust_beta = robust_gamlss.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import gamma\n",
    "\n",
    "# quantiles = gamma.ppf([0.025, 0.975], robust_alpha, scale = 1/robust_beta)\n",
    "# quantiles\n",
    "\n",
    "# robust_lower = quantiles[:,0]\n",
    "# robust_upper = quantiles[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(X_test_sorted, y_test_sorted, \"o\", markersize = 1)\n",
    "# plt.plot(X_test_sorted, robust_lower[sorted_indices], color = \"red\")\n",
    "# plt.plot(X_test_sorted, robust_upper[sorted_indices], color = \"red\")\n",
    "# plt.ylim((0,4000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
